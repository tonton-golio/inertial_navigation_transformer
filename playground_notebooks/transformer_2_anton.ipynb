{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "folder_path = '/Users/antongolles/Documents/uni/masters/myMasters/applied_machine_learning/inertial_navigation_transformer/data/data_from_RoNIN/train_dataset_1/'\n",
    "\n",
    "# Define constant variables\n",
    "FOLDER_PATH = '/Users/antongolles/Documents/uni/masters/myMasters/applied_machine_learning/inertial_navigation_transformer/data/data_from_RoNIN/train_dataset_1/'\n",
    "\n",
    "N_TRAIN = 60_000\n",
    "N_VAL_FRACTION = .2\n",
    "N_TEST = 25_000\n",
    "NUM_DATASETS = 10\n",
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.003\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Set input and output features\n",
    "INPUT_FEATURES = ['synced/gyro', 'synced/acce', 'synced/magnet']\n",
    "OUTPUT_FEATURES = ['pose/tango_pos']\n",
    "\n",
    "params = {'N_train': N_TRAIN, \n",
    "        'N_test': N_TEST,\n",
    "        'seq_len': SEQ_LEN, \n",
    "        'input': INPUT_FEATURES,\n",
    "        'output': OUTPUT_FEATURES, \n",
    "        'normalize': False,\n",
    "        'verbose': False, \n",
    "        'num_datasets': NUM_DATASETS,\n",
    "        'random_start': True,\n",
    "        'overlap': 0,\n",
    "        'include_theta': False}\n",
    "\n",
    "X_train, y_train, X_test, y_test, col_locations = load_split_data(folder_path=FOLDER_PATH, **params)\n",
    "\n",
    "print('X_train shape: ', X_train.shape, '\\ny_train shape: ', y_train.shape, '\\nX_test shape: ', X_test.shape, '\\ny_test shape: ', y_test.shape)\n",
    "\n",
    "def preprocess_data(X_train, y_train, X_test, y_test, col_locations):\n",
    "    # for y, subtract the first value from all values\n",
    "\n",
    "# preprocess data\n",
    "X_train, y_train, X_test, y_test = preprocess_data(X_train, y_train, X_test, y_test, col_locations)\n",
    "\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "input_features = ['synced/gyro']\n",
    "output_features = ['pose/tango_pos']\n",
    "if include_acc:\n",
    "    input_features.append('synced/acce')\n",
    "if include_mag:\n",
    "    input_features.append('synced/magnet')\n",
    "if include_lin_acc:\n",
    "    input_features.append('synced/linacce')\n",
    "if include_ori:\n",
    "    output_features.append('pose/tango_ori')\n",
    "\n",
    "\n",
    "\n",
    "params = {'N_train': Ntrain, \n",
    "        'N_test': Ntest,\n",
    "        'seq_len': seq_len, \n",
    "        'input': input_features,\n",
    "        'output': output_features, \n",
    "        'normalize': normalize,\n",
    "        'verbose': False, \n",
    "        'num_datasets':num_datasets,\n",
    "        'random_start':random_start,\n",
    "        'overlap': overlap,\n",
    "        'include_theta': include_theta,}\n",
    "X_train, y_train, X_test, y_test, col_locations = load_split_data(folder_path=folder_path, **params)\n",
    "print(col_locations)\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "d_model = 7\n",
    "nhead = 2\n",
    "dim_feedforward = 120\n",
    "learning_rate = 0.003\n",
    "num_epochs = 10\n",
    "hidden_size = 64\n",
    "batch_size = 1\n",
    "num_layers = 3\n",
    "\n",
    "# dataset\n",
    "dataset_train = torch.utils.data.TensorDataset( torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float() )\n",
    "# dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# model\n",
    "# set up a tansformer model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, batch_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "        # Config variables\n",
    "        self.d_model = 7\n",
    "        self.nhead = 1\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = hidden_size * 4\n",
    "\n",
    "        self.fc_get_next_quat = nn.Sequential(\n",
    "            nn.Linear(4*4 + 4, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Linear(hidden_size, 4)\n",
    "        )\n",
    "\n",
    "        self.quat_cleaner = nn.Sequential(\n",
    "            nn.Linear(13, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 4)\n",
    "        )\n",
    "\n",
    "        #self.conv1 = nn.Conv1d(self.d_model, self.d_model, kernel_size=3, padding=1)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=self.d_model, nhead=self.nhead, num_encoder_layers=self.num_layers, num_decoder_layers=self.num_layers, dim_feedforward=self.dim_feedforward, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(7, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(hidden_size*2, hidden_size*4),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(hidden_size*4, hidden_size*4),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(hidden_size*4, hidden_size*2),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "        # set all the weights to be the same\n",
    "        # for p in self.parameters():\n",
    "        #     if p.dim() > 1:\n",
    "        #         nn.init.xavier_uniform_(p)\n",
    "        # #set all the weights to be 0\n",
    "        # for p in self.parameters():\n",
    "        #     p.data.fill_(0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # {'input': {'synced/gyro': (0, 3), 'synced/acce': (3, 6), 'synced/magnet': (6, 9), 'theta': (9, 25)}, \n",
    "        gyro =  x[:,:,0:3]; theta = x[:,:,9:25]\n",
    "        magnet = x[:,:,3:6]\n",
    "        acce = x[:,:,6:9]\n",
    "        \n",
    "        # get initial orientation\n",
    "        R = [get_body2world_rot(m[0].numpy(),a[0].numpy()) for m,a in zip(magnet, acce)]\n",
    "        quat_start = [rotation_matrix_2_quaternion(np.linalg.inv(r)) for r in R]\n",
    "        quat_start = torch.tensor(quat_start).float().squeeze().unsqueeze(0)\n",
    "        quats = [quat_start]\n",
    "        # get all the quaternions\n",
    "        for i in range(1, len(theta[0])):\n",
    "            quat = quats[i-1].squeeze()\n",
    "            theta_ = theta.squeeze()[i-1:i]#\n",
    "            quat_next_ana = theta_.reshape(4,4) @ quat\n",
    "            catted = torch.cat((quat_next_ana, theta_.squeeze()))\n",
    "            next_quat = self.fc_get_next_quat(catted)\n",
    "            next_quat = next_quat / torch.norm(next_quat)\n",
    "            quats.append(next_quat.reshape(1,4) )\n",
    "        quats = torch.stack(quats).squeeze().unsqueeze(0)\n",
    "        \n",
    "\n",
    "        # clean quats \n",
    "        #print('quat shape', quats.shape, 'gyro shape', gyro.shape, 'acce shape', acce.shape, 'magnet shape', magnet.shape)\n",
    "        quats = self.quat_cleaner(torch.cat((quats, gyro, acce, magnet), dim=2))\n",
    "        quats = quats / torch.norm(quats, dim=1)\n",
    "        quats = quats.squeeze()\n",
    "        # rotate the acce and magnet\n",
    "        #print(acce.shape)\n",
    "        acce_and_quat = torch.cat((acce.squeeze(), quats), dim=-1).unsqueeze(0)\n",
    "        #print(acce_and_quat.shape)\n",
    "        trans_out = self.transformer(acce_and_quat, acce_and_quat)\n",
    "        #print(trans_out.shape)\n",
    "        \n",
    "        final_out = self.fc_out(trans_out)\n",
    "        #print(final_out.shape)\n",
    "        # select first 3 elements\n",
    "        final_out = final_out[:,:,0:3]\n",
    "        return final_out\n",
    "\n",
    "# Training Loop\n",
    "net = Net(input_size=X_train.shape[2], hidden_size=hidden_size, output_size=7, num_layers=num_layers, batch_size=batch_size)\n",
    "optim_ = optim.AdamW(params=net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim_, step_size=0.1, gamma=0.05) # Add a learning rate scheduler\n",
    "criterion = nn.MSELoss()\n",
    "losses = []\n",
    "from tqdm import tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(dataloader):\n",
    "        X_, y_ = data\n",
    "        optim_.zero_grad()\n",
    "        output =  net(X_)\n",
    "        loss = criterion(output, y_)\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(net.parameters(), max_norm=10)  # Uncommented gradient clipping\n",
    "        optim_.step()\n",
    "        scheduler.step()  # Added step for learning rate scheduler\n",
    "        running_loss += loss.item()\n",
    "        #print(loss.item() / batch_size)\n",
    "    print(f'epoch: {epoch}, loss: {running_loss}')\n",
    "    losses.append(running_loss)\n",
    "\n",
    "\n",
    "# plot the losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n",
    "short_name = 'tiny'\n",
    "plt.savefig(f'../figures/{short_name}_transformer_{d_model}_{nhead}_{num_layers}_{dim_feedforward}.png', dpi=300)\n",
    "\n",
    "# test\n",
    "N_test = torch.from_numpy(X_test[:1]).float()\n",
    "\n",
    "# set the model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = net(N_test)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "accumulated_preds = []\n",
    "for pred in predictions:\n",
    "    accumulated_preds.append(pred + accumulated_preds[-1][-1] if accumulated_preds else pred)\n",
    "predictions = np.array(accumulated_preds)\n",
    "predictions = predictions.reshape(-1, 3)\n",
    "# plot the predictions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(predictions[:,0], predictions[:,1], label='predictions')\n",
    "ax[0].legend()\n",
    "y_test_reshaped = y_test.reshape(-1, 3)\n",
    "y_test_reshaped -= y_test_reshaped[0]\n",
    "ax[1].plot(y_test_reshaped[:,0], y_test_reshaped[:,1], label='truth')\n",
    "ax[1].legend()\n",
    "\n",
    "# save the model\n",
    "import datetime\n",
    "timestamp_ = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "torch.save(net.state_dict(), f'../transformer_big_{timestamp_}.pt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
