{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '.../data.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m     11\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.../data.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 13\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60_000\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive - University of Copenhagen\\GitHub\\inertial_navigation_transformer\\playground_notebooks\\..\\utils.py:52\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path, verbose)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/antongolles/Documents/work/Rokoko/velocity_est/data/data_from_RoNIN/train_dataset_1/a000_1/data.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hdf_file:\n\u001b[0;32m     53\u001b[0m         data_dict \u001b[38;5;241m=\u001b[39m get_all_datasets(hdf_file)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    525\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    527\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    528\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    529\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    530\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    531\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    532\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 533\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    225\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 226\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    228\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '.../data.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "data_path = '../data.hdf5'\n",
    "\n",
    "data_dict = load_data(data_path, verbose=True)\n",
    "\n",
    "start=0\n",
    "stop=60_000\n",
    "\n",
    "ori =data_dict['pose/tango_ori'][start:stop]  \n",
    "w = data_dict['synced/gyro'][start:stop]\n",
    "a = data_dict['synced/acce'][start:stop]\n",
    "m = data_dict['synced/magnet'][start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET PARAMETERS \n",
    "w = data_dict['synced/gyro']\n",
    "Ntot = w.shape[0]\n",
    "\n",
    "print(\"Total no. of data points: \", Ntot)\n",
    "# Set the number of points used to train network\n",
    "N = 20_000\n",
    "# set the no. of points used to valiate on\n",
    "Nval = 1000\n",
    "assert(N + Nval < Ntot)\n",
    "\n",
    "n_seen = N\n",
    "\n",
    "# Set whether to include acceleration and magnetic data when training \n",
    "include_acc = True\n",
    "include_mag = False\n",
    "\n",
    "# Set no. of layers in model\n",
    "Nlayers = 30\n",
    "# Set neurons per layer in model\n",
    "hidden_size = 128\n",
    "# Set learning rate\n",
    "learning_rate = 0.0001\n",
    "# Set loss function\n",
    "criterion = nn.L1Loss()\n",
    "# set n_epochs\n",
    "num_epochs = 600\n",
    "# Set how many observations to combine into one datapoint\n",
    "group_num = 30 # EFFECT: By grouping, we are only giving the model 1 true orientation for every group_num measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict angles analytically\n",
    "q_gt = data_dict['pose/tango_ori'][N:N + Nval]\n",
    "dt = data_dict['synced/time'][N+1] - data_dict['synced/time'][N]\n",
    "\n",
    "q_pred_ana = np.zeros((Nval,4))\n",
    "q_pred_ana[0] = q_gt[0]\n",
    "factor= .0025                                # has been set arbitrarily\n",
    "for i in range(1,Nval):\n",
    "    q_pred_ana[i] = Theta(w[N + i]*factor, dt=dt)@q_pred_ana[i-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "def plot_quat(q_gts, q_preds, xs, names = ['analytical', 'NN']):\n",
    "    if type(q_preds) is not list:\n",
    "        q_preds = [q_preds]\n",
    "    if type(q_gts) is not list:\n",
    "        q_gts = [q_gts]\n",
    "    if type(xs) is not list:\n",
    "        xs = [xs]\n",
    "    \n",
    "    fig, ax = plt.subplots(len(q_preds), 4, figsize=(12, 4*len(q_preds)))\n",
    "    ax = ax.reshape(-1, 4)\n",
    "    idx = 0\n",
    "    for q_pred, q_gt,x, name in zip(q_preds, q_gts,xs, names):\n",
    "        print('q_pred', q_pred.shape, 'q_gt', q_gt.shape, 'x', x.shape)\n",
    "        ax[idx, 0].plot(x, q_gt[:,0],)\n",
    "        ax[idx, 0].plot(x, q_pred[:,0],)\n",
    "        ax[idx, 1].plot(x, q_gt[:,1],)\n",
    "        ax[idx, 1].plot(x, q_pred[:,1],)\n",
    "        ax[idx, 2].plot(x, q_gt[:,2],)\n",
    "        ax[idx, 2].plot(x, q_pred[:,2],)\n",
    "        ax[idx, 3].plot(x, q_gt[:,3],label='gt')\n",
    "        ax[idx, 3].plot(x, q_pred[:,3],label=f'pred')\n",
    "        \n",
    "        ax[idx, 0].set_title(name)\n",
    "        idx += 1\n",
    "    ax[0, 3].legend()\n",
    "    fig.suptitle('Quaternion Update test')\n",
    "    plt.tight_layout()\n",
    "\n",
    "#plot_quat(q_gt, q_pred_ana, np.arange(N, N + Nval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(start=0, stop=1500, group_num=40, include_mag = False, include_acc = False):\n",
    "\n",
    "    ori =data_dict['pose/tango_ori'][start:stop]  \n",
    "    w = data_dict['synced/gyro'][start:stop]\n",
    "    a = data_dict['synced/acce'][start:stop]\n",
    "    m = data_dict['synced/magnet'][start:stop]\n",
    "    #print shapes\n",
    "    print('ori.shape', ori.shape, 'w.shape', w.shape, 'a.shape', a.shape, 'm.shape', m.shape)\n",
    "\n",
    "    # Now we group all N datapoints into N/group_num rows. To do this, we discard rows to \n",
    "    # ensure that N is divisble by group_num_rows\n",
    "    w = w[:-(w.shape[0]%group_num)] if w.shape[0]%group_num != 0 else w\n",
    "    a = a[:-(a.shape[0]%group_num)] if a.shape[0]%group_num != 0 else a\n",
    "    m = m[:-(m.shape[0]%group_num)] if m.shape[0]%group_num != 0 else m\n",
    "    ori = ori[:-(ori.shape[0]%group_num)] if ori.shape[0]%group_num != 0 else ori\n",
    "\n",
    "    # reshape\n",
    "    w_10 = w.reshape(-1,group_num*w.shape[1])\n",
    "    a_10 = a.reshape(-1,group_num*a.shape[1])\n",
    "    m_10 = m.reshape(-1,group_num*m.shape[1])\n",
    "    ori_10 = ori.reshape(-1,group_num*ori.shape[1])\n",
    "    \n",
    "    print('w_10.shape', w_10.shape)\n",
    "    print('m_10.shape', m_10.shape)\n",
    "    print('a_10.shape ', a_10.shape)\n",
    "    print('ori_10.shape', ori_10.shape)\n",
    "\n",
    "    if include_mag and include_acc:\n",
    "        X = np.hstack([w_10, a_10, m_10, ori_10[:,:4] ])\n",
    "    elif include_mag:\n",
    "        X = np.hstack([w_10, m_10, ori_10[:,:4] ])\n",
    "    elif include_acc:\n",
    "        X = np.hstack([w_10, a_10, ori_10[:,:4] ])\n",
    "    else:\n",
    "        X = np.hstack([w_10, ori_10[:,:4] ])\n",
    "\n",
    "    y = ori_10[:,-4:]  \n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = get_data(start=0, stop=n_seen, group_num=group_num, include_acc=include_acc, include_mag=include_mag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a neural network with 3 inputs and 16 outputs\n",
    "# the 3 inputs are the 3 components of the angular velocity\n",
    "# the 16 outputs are the 16 elements of the 4x4 matrix Theta(w)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=13, output_size=16, hidden_size=16, num_layers=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.fc_input = nn.Linear(input_size, hidden_size)\n",
    "        hidden_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.hidden_layers = nn.ModuleList(hidden_layers)\n",
    "\n",
    "        self.fc_output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = F.relu(self.fc_input(x))\n",
    "        for i in range(self.num_layers):\n",
    "            x = F.relu(self.hidden_layers[i](x))\n",
    "            \n",
    "        x = self.fc_output(x)\n",
    "        return x\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "\n",
    "net = Net(input_size=X.shape[1], output_size=4, hidden_size=hidden_size, num_layers=Nlayers)\n",
    "\n",
    "# mount model to device\n",
    "net.to(device)\n",
    "# number of params\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, shuffle=False)\n",
    "\n",
    "# convert to torch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "# dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "t_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    n_minibatches = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        n_minibatches += 1\n",
    "    print('Epoch %d, loss: %.5f' % (epoch+1, running_loss/n_minibatches))\n",
    "\n",
    "    # test\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        n_minibatches = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            n_minibatches += 1\n",
    "        print('Test loss: %.5f' % (running_loss/n_minibatches))\n",
    "\n",
    "t_end = time.time()\n",
    "print(\"Training time: \", t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing unseen         \n",
    "\n",
    "n_unseen = Nval\n",
    "\n",
    "start, end = n_seen, n_seen+n_unseen\n",
    "#start, end = 0, n_unseen\n",
    "\n",
    "\n",
    "X_unseen, y_unseen = get_data(start=start, stop=end, group_num = group_num, include_acc=include_acc, include_mag=include_mag)\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "if 1:\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_unseen)):\n",
    "            if i > 0:\n",
    "                X_unseen[i, -4:] = y_pred[i-1]\n",
    "\n",
    "            inputs = torch.from_numpy(X_unseen[i]).float()\n",
    "            # mount to gpu if using\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = net(inputs)\n",
    "            # copy to cpu and store prediction\n",
    "            outputs_cpu = torch.Tensor.cpu(outputs)\n",
    "            y_pred.append(outputs_cpu.numpy())\n",
    "\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "q_gts = [q_gt, y_unseen]\n",
    "q_preds = [q_pred_ana, y_pred]\n",
    "xs = [np.arange(len(q_gt)), np.arange(len(y_unseen))]\n",
    "plot_quat(q_gts, q_preds, xs, names = ['analytical', 'NN'])\n",
    "plt.savefig('../assets/quat_nn.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = torch.load('load/from/path/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
