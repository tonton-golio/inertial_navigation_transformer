{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def get_all_datasets(hdf_file):\n",
    "    \"\"\"\n",
    "    Function to return all datasets from an HDF5 file.\n",
    "\n",
    "    Args:\n",
    "    hdf_file : h5py.File object\n",
    "\n",
    "    Returns:\n",
    "    datasets : dict\n",
    "        Dictionary with dataset names as keys and numpy arrays as values.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "\n",
    "    def collect_datasets(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            datasets[name] = obj[:]\n",
    "\n",
    "    hdf_file.visititems(collect_datasets)\n",
    "    return datasets\n",
    "\n",
    "def nice_dict_contents(data_dict, print_keys=False):\n",
    "    \"\"\"\n",
    "    Function to print the contents of the dictionary in a hierarchical manner.\n",
    "    \n",
    "    Args:\n",
    "    data_dict : dict\n",
    "        Dictionary containing the data.\n",
    "    print_keys : bool, optional\n",
    "        Flag indicating whether to print the keys. Defaults to False.\n",
    "    \"\"\"\n",
    "    outer_keys = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for key in data_dict.keys():\n",
    "        split_key = key.split('/')\n",
    "        \n",
    "        if len(split_key) == 2:\n",
    "            outer, inner = split_key\n",
    "            outer_keys[outer][inner]\n",
    "        else:\n",
    "            outer, middle, inner = split_key\n",
    "            outer_keys[outer][middle].append(inner)\n",
    "            \n",
    "    if print_keys:\n",
    "        print('CONTENTS OF HDF5 FILE:')\n",
    "        for outer_key, outer_value in outer_keys.items():\n",
    "            print(outer_key)\n",
    "            for inner_key, inner_values in outer_value.items():\n",
    "                print('\\t', inner_key)\n",
    "                print('\\t\\t', ', '.join(inner_values))\n",
    "\n",
    "def load_data(file_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to load the data from a given file path.\n",
    "\n",
    "    Args:\n",
    "    file_path : str\n",
    "        Path to the data file.\n",
    "    verbose : bool, optional\n",
    "        If True, print the contents of the file.\n",
    "\n",
    "    Returns:\n",
    "    data_dict : dict\n",
    "        Dictionary with dataset names as keys and numpy arrays as values.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as hdf_file:\n",
    "        data_dict = get_all_datasets(hdf_file)\n",
    "    \n",
    "    if verbose:\n",
    "        nice_dict_contents(data_dict, print_keys=True)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    \"\"\"\n",
    "    Function to create sequences from the input data.\n",
    "\n",
    "    Args:\n",
    "    X : numpy array\n",
    "        Input data.\n",
    "    y : numpy array\n",
    "        Target data.\n",
    "    seq_length : int\n",
    "        Sequence length.\n",
    "\n",
    "    Returns:\n",
    "    X_seq, y_seq : numpy arrays\n",
    "        Sequenced input and target data.\n",
    "    \"\"\"\n",
    "    X_seq = [X[i:i+seq_length] for i in range(0, len(X) - seq_length + 1, seq_length)]\n",
    "    y_seq = [y[i:i+seq_length] for i in range(0, len(y) - seq_length + 1, seq_length)]\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def load_much_data(N_train, N_test, folder_path, columns_X, columns_y, seq_length=1, verbose=False, num_datasets=1):\n",
    "    \"\"\"\n",
    "    Function to load data from multiple HDF5 files.\n",
    "\n",
    "    Args:\n",
    "    Ntrain : int\n",
    "        Number of training instances.\n",
    "    Nval : int\n",
    "        Number of validation instances.\n",
    "    folder_path : str\n",
    "        Path to the data directory.\n",
    "    columns : list\n",
    "        List of column names.\n",
    "\n",
    "    Returns:\n",
    "    data_dict : dict\n",
    "        Dictionary with dataset names as keys and numpy arrays as values.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'X-train': {key: None for key in columns_X},\n",
    "        'y-train': {key: None for key in columns_y},\n",
    "        'X-test': {key: None for key in columns_X},\n",
    "        'y-test': {key: None for key in columns_y},\n",
    "    }\n",
    "    Nloaded_points = 0\n",
    "    # get control of directories\n",
    "    dirs = os.listdir(folder_path)\n",
    "    if '.DS_Store' in dirs: dirs.remove('.DS_Store') # remove .DS_Store if present\n",
    "    test_dir = dirs[:1]\n",
    "    train_dirs = dirs[1:num_datasets+1]\n",
    "    print(f'using {test_dir} for testing and the remaining ({len(train_dirs)}) for training')\n",
    "    \n",
    "\n",
    "    N_points_per_dir = max(int(N_train/len(train_dirs)), seq_length)\n",
    "    N_points_per_dir = N_points_per_dir - N_points_per_dir % seq_length\n",
    "    n_dirs_to_use = int(N_train/N_points_per_dir)\n",
    "    train_dirs = train_dirs[:n_dirs_to_use+1]\n",
    "    N_points = N_points_per_dir * len(train_dirs)\n",
    "    \n",
    "    print(f'Loading a total of {N_train}, with {N_points_per_dir} points from each of {len(train_dirs)} directories')\n",
    "    for dir in dirs:\n",
    "        file_path = os.path.join(folder_path, dir, 'data.hdf5')\n",
    "        if verbose: print('Loading file:', file_path)\n",
    "        with h5py.File(file_path, \"r\") as hdf_file:\n",
    "            new_data_dict = load_data(file_path)\n",
    "            Nloaded_points += N_points_per_dir\n",
    "        if dir in test_dir:\n",
    "\n",
    "            for key in columns_X:\n",
    "                if data['X-test'][key] is None:\n",
    "                    data['X-test'][key] = new_data_dict[key][:N_test]\n",
    "                else:\n",
    "                    data['X-test'][key] = np.vstack([data['X-test'][key], new_data_dict[key][:N_test]])\n",
    "            for key in columns_y:\n",
    "                if data['y-test'][key] is None:\n",
    "                    data['y-test'][key] = new_data_dict[key][:N_test]\n",
    "                else:\n",
    "                    data['y-test'][key] = np.vstack([data['y-test'][key], new_data_dict[key][:N_test]])\n",
    "        elif dir in train_dirs:\n",
    "            for key in columns_X:\n",
    "                if data['X-train'][key] is None:\n",
    "                    data['X-train'][key] = new_data_dict[key][:N_points_per_dir]\n",
    "                else:\n",
    "                    data['X-train'][key] = np.vstack([data['X-train'][key], new_data_dict[key][:N_points_per_dir]])\n",
    "            for key in columns_y:\n",
    "                if data['y-train'][key] is None:\n",
    "                    data['y-train'][key] = new_data_dict[key][:N_points_per_dir]\n",
    "                else:\n",
    "                    data['y-train'][key] = np.vstack([data['y-train'][key], new_data_dict[key][:N_points_per_dir]])\n",
    "        if Nloaded_points >= N_points:\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_features(X, scaler=None):\n",
    "    \"\"\"\n",
    "    Function to normalize the features.\n",
    "\n",
    "    Args:\n",
    "    X : numpy array\n",
    "        Input data.\n",
    "\n",
    "    Returns:\n",
    "    X_normalized : numpy array\n",
    "        Normalized input data.\n",
    "    scaler : sklearn.preprocessing.StandardScaler\n",
    "        The scaler used for normalization. Useful for inverse transformation.\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    return X_normalized, scaler\n",
    "\n",
    "def load_split_data(folder_path='C:\\\\Users\\\\Simon Andersen\\\\Documents\\\\Uni\\\\KS6\\\\AppliedML\\\\Project 2\\\\train_dataset_1', **kwargs):\n",
    "    \"\"\"\n",
    "    Function to load, split, and preprocess the data.\n",
    "\n",
    "    Args:\n",
    "    folder_path : str\n",
    "        Path to the data directory.\n",
    "    **kwargs : other parameters to control the data loading and processing.\n",
    "            - N_points: number of training instances\n",
    "            - seq_len: sequence length, must be >= 1\n",
    "            - input: list of input features, e.g. ['pose/tango_ori', 'pose/tango_pos', 'synced/gyro']\n",
    "            - output: list of output features, e.g. ['pose/tango_ori']\n",
    "            - normalize: boolean, whether to normalize the data or not.\n",
    "            - shuffle: boolean, whether to shuffle the data or not.\n",
    "            - verbose: boolean, whether to print information about the data or not.\n",
    "\n",
    "    Returns:\n",
    "    X_reshaped : numpy array\n",
    "        The processed and reshaped input data.\n",
    "    y_reshaped : numpy array\n",
    "        The processed and reshaped target data.\n",
    "    \"\"\"\n",
    "    params = {'N_train': 1000,'N_test': 100, 'seq_len': 10, 'input': [], 'output': [], 'normalize': False, 'shuffle': True, 'verbose': True, 'num_datasets':1}\n",
    "    params.update(kwargs)\n",
    "\n",
    "    allowed_columns = [\n",
    "        'pose/ekf_ori', 'pose/tango_ori', 'pose/tango_pos',\n",
    "        'synced/acce', 'synced/game_rv', 'synced/grav', 'synced/gyro', 'synced/gyro_uncalib', 'synced/linacce', 'synced/magnet', 'synced/rv',\n",
    "        'raw/imu/acce', 'raw/imu/game_rv', 'raw/imu/gps', 'raw/imu/gravity', 'raw/imu/gyro', 'raw/imu/gyro_uncalib', 'raw/imu/linacce', 'raw/imu/magnet', 'raw/imu/magnetic_rv', 'raw/imu/pressure', 'raw/imu/rv', 'raw/imu/step', 'raw/imu/wifi_address', 'raw/imu/wifi_values',\n",
    "        'raw/tango/acce', 'raw/tango/game_rv', 'raw/tango/gps', 'raw/tango/gravity', 'raw/tango/gyro', 'raw/tango/gyro_uncalib', 'raw/tango/linacce', 'raw/tango/magnet', 'raw/tango/magnetic_rv', 'raw/tango/pressure', 'raw/tango/rv', 'raw/tango/step', 'raw/tango/tango_adf_pose', 'raw/tango/tango_pose', 'raw/tango/wifi_address', 'raw/tango/wifi_values',\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # make sure columns are in allowed_columns\n",
    "    columns = params['output'] + params['input']\n",
    "    for column in columns:\n",
    "        if column not in allowed_columns:\n",
    "            raise NameError(f'ERROR: Column \"{column}\" not in allowed columns: {allowed_columns}')\n",
    "\n",
    "    data = load_much_data(folder_path=folder_path, \n",
    "                            columns_X=params['input'], columns_y=params['output'], \n",
    "                            N_train=params['N_train'], N_test=params['N_test'],\n",
    "                            verbose=params['verbose'], \n",
    "                            seq_length=params['seq_len'], \n",
    "                            num_datasets=params['num_datasets'])\n",
    "    \n",
    "    \n",
    "\n",
    "    X_train = np.hstack([data['X-train'][key] for key in params['input']])\n",
    "    y_train = np.hstack([data['y-train'][key] for key in params['output']])\n",
    "    X_test = np.hstack([data['X-test'][key] for key in params['input']])\n",
    "    y_test = np.hstack([data['y-test'][key] for key in params['output']])\n",
    "    if params['normalize']:\n",
    "        X_train, scaler = normalize_features(X_train)\n",
    "        # use the same scaler for test data\n",
    "        X_test, _ = normalize_features(X_test, scaler=scaler)\n",
    "\n",
    "        \n",
    "    X_train_reshaped, y_train_reshaped = create_sequences(X_train, y_train, seq_length=params['seq_len'])\n",
    "    X_test_reshaped, y_test_reshaped = create_sequences(X_test, y_test, seq_length=params['seq_len'])\n",
    "\n",
    "    return X_train_reshaped, y_train_reshaped, X_test_reshaped, y_test_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ['a017_1'] for testing and the remaining (6) for training\n",
      "Loading a total of 2000, with 330 points from each of 6 directories\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/antongolles/Documents/uni/masters/myMasters/applied_machine_learning/inertial_navigation_transformer/data/data_from_RoNIN/train_dataset_1/'\n",
    "\n",
    "params = {'N_train': 2000, 'N_test':500, 'seq_len': 15, \n",
    "          'input': ['pose/tango_ori', 'pose/tango_pos', 'synced/gyro'], \n",
    "          'output': ['pose/tango_ori'], \n",
    "          'normalize': False, 'verbose': False, 'num_datasets':6}\n",
    "X_train, y_train, X_test, y_test = load_split_data(folder_path=folder_path, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (36, 11, 10) y train shape:  (36, 11, 4) X test shape:  (9, 11, 10) y test shape:  (9, 11, 4)\n"
     ]
    }
   ],
   "source": [
    "print('X train shape: ', X_train.shape, 'y train shape: ', y_train.shape, 'X test shape: ', X_test.shape, 'y test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_length, overlap=1):\n",
    "    \"\"\"\n",
    "    Function to create sequences from the input data.\n",
    "\n",
    "    Args:\n",
    "    X : numpy array\n",
    "        Input data.\n",
    "    y : numpy array\n",
    "        Target data.\n",
    "    seq_length : int\n",
    "        Sequence length.\n",
    "    overlap : int, optional\n",
    "        Overlap between sequences. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    X_seq, y_seq : numpy arrays\n",
    "        Sequenced input and target data.\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(0, len(X) - seq_length + 1, seq_length-overlap):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i:i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seq:  [[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]\n",
      "  [ 9 10 11]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]\n",
      "  [18 19 20]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(27).reshape(-1,3)\n",
    "y = np.arange(27).reshape(-1,1)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X, y, seq_length=4, overlap=1)\n",
    "print('X_seq: ', X_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
